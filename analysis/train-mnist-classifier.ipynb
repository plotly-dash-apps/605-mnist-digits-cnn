{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Simpler Models\n",
    "https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The “hello world” of object recognition for machine learning and deep learning is the MNIST dataset for handwritten digit recognition.\n",
    "\n",
    "In this post you will discover how to develop a deep learning model to achieve near state of the art performance on the MNIST handwritten digit recognition task in Python using the Keras deep learning library.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "    How to load the MNIST dataset in Keras.\n",
    "    How to develop and evaluate a baseline neural network model for the MNIST problem.\n",
    "    How to implement and evaluate a simple Convolutional Neural Network for MNIST.\n",
    "    How to implement a close to state-of-the-art deep learning model for MNIST.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the MNIST Handwritten Digit Recognition Problem\n",
    "\n",
    "The MNIST problem is a dataset developed by Yann LeCun, Corinna Cortes and Christopher Burges for evaluating machine learning models on the handwritten digit classification problem.\n",
    "\n",
    "The dataset was constructed from a number of scanned document dataset available from the National Institute of Standards and Technology (NIST). This is where the name for the dataset comes from, as the Modified NIST or MNIST dataset.\n",
    "\n",
    "Images of digits were taken from a variety of scanned documents, normalized in size and centered. This makes it an excellent dataset for evaluating models, allowing the developer to focus on the machine learning with very little data cleaning or preparation required.\n",
    "\n",
    "Each image is a 28 by 28 pixel square (784 pixels total). A standard split of the dataset is used to evaluate and compare models, where 60,000 images are used to train a model and a separate set of 10,000 images are used to test it.\n",
    "\n",
    "It is a digit recognition task. As such there are 10 digits (0 to 9) or 10 classes to predict. Results are reported using prediction error, which is nothing more than the inverted classification accuracy.\n",
    "\n",
    "Excellent results achieve a prediction error of less than 1%. State-of-the-art prediction error of approximately 0.2% can be achieved with large Convolutional Neural Networks. There is a listing of the state-of-the-art results and links to the relevant papers on the MNIST and other datasets on Rodrigo Benenson’s webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installs & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# numpy -> linear algebra & matrix operations\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras deep learning library provides a convenience method for loading the MNIST dataset.\n",
    "\n",
    "The dataset is downloaded automatically the first time this function is called and is stored in your home directory in ~/.keras/datasets/mnist.pkl.gz as a 15MB file.\n",
    "\n",
    "This is very handy for developing and testing deep learning models.\n",
    "\n",
    "To demonstrate how easy it is to load the MNIST dataset, we will first write a little script to download and visualize the first 4 images in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load (downloaded if needed) the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example loads the MNIST train and test dataset and prints their shape.\n",
    "\n",
    "We can see that there are 60,000 examples in the training dataset and 10,000 in the test dataset and that images are indeed square with 28×28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: X=(60000, 28, 28), y=(60000,)\n",
      "Test: X=(10000, 28, 28), y=(10000,)\n"
     ]
    }
   ],
   "source": [
    "# summarize loaded dataset\n",
    "print('Train: X=%s, y=%s' % (X_train.shape, y_train.shape))\n",
    "print('Test: X=%s, y=%s' % (X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot of the first few images in the dataset is also created showing the natural handwritten nature of the images to be classified.\n",
    "Plot of a Subset of Images From the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot 4 images as gray scale\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model (Feed-Forward MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we really need a complex model like a convolutional neural network to get the best results with MNIST?\n",
    "\n",
    "You can get very good results using a very simple neural network model with a single hidden layer. In this section we will create a simple multi-layer perceptron model that achieves an error rate of 1.74%. We will use this as a baseline for comparing more complex convolutional neural network models.\n",
    "\n",
    "Let’s start off by importing the classes and functions we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the MNIST dataset using the Keras helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset is structured as a 3-dimensional array of instance, image width and image height. For a multi-layer perceptron model we must reduce the images down into a vector of pixels. In this case the 28×28 sized images will be 784 pixel input values.\n",
    "\n",
    "We can do this transform easily using the reshape() function on the NumPy array. We can also reduce our memory requirements by forcing the precision of the pixel values to be 32 bit, the default precision used by Keras anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten 28*28 images to a 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape((X_train.shape[0], num_pixels)).astype('float32')\n",
    "X_test = X_test.reshape((X_test.shape[0], num_pixels)).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pixel values are gray scale between 0 and 255. It is almost always a good idea to perform some scaling of input values when using neural network models. Because the scale is well known and well behaved, we can very quickly normalize the pixel values to the range 0 and 1 by dividing each value by the maximum of 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "print(round(X_train.max(),5))\n",
    "print(round(X_test.max(),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the output variable is an integer from 0 to 9. This is a multi-class classification problem. As such, it is good practice to use a one hot encoding of the class values, transforming the vector of class integers into a binary matrix.\n",
    "\n",
    "We can easily do this using the built-in np_utils.to_categorical() helper function in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to create our simple neural network model. We will define our model in a function. This is handy if you want to extend the example later and try and get a better score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(num_pixels, \n",
    "                    input_dim=num_pixels, \n",
    "                    kernel_initializer='normal', \n",
    "                    activation='relu'))\n",
    "\tmodel.add(Dense(num_classes, \n",
    "                    kernel_initializer='normal', \n",
    "                    activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='\n",
    "                  optimizer='adam', categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is a simple neural network with one hidden layer with the same number of neurons as there are inputs (784). A rectifier activation function is used for the neurons in the hidden layer.\n",
    "\n",
    "A softmax activation function is used on the output layer to turn the outputs into probability-like values and allow one class of the 10 to be selected as the model’s output prediction. Logarithmic loss is used as the loss function (called categorical_crossentropy in Keras) and the efficient ADAM gradient descent algorithm is used to learn the weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 11:53:13.214968: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = baseline_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit and evaluate the model. The model is fit over 10 epochs with updates every 200 images. The test data is used as the validation dataset, allowing you to see the skill of the model as it trains. A verbose value of 2 is used to reduce the output to one line for each training epoch.\n",
    "\n",
    "\n",
    "Running the example might take a few minutes when run on a CPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 - 2s - loss: 0.2778 - accuracy: 0.9202 - val_loss: 0.1387 - val_accuracy: 0.9580 - 2s/epoch - 8ms/step\n",
      "Epoch 2/10\n",
      "300/300 - 2s - loss: 0.1089 - accuracy: 0.9684 - val_loss: 0.0987 - val_accuracy: 0.9685 - 2s/epoch - 6ms/step\n",
      "Epoch 3/10\n",
      "300/300 - 2s - loss: 0.0699 - accuracy: 0.9799 - val_loss: 0.0824 - val_accuracy: 0.9749 - 2s/epoch - 6ms/step\n",
      "Epoch 4/10\n",
      "300/300 - 2s - loss: 0.0501 - accuracy: 0.9855 - val_loss: 0.0731 - val_accuracy: 0.9777 - 2s/epoch - 6ms/step\n",
      "Epoch 5/10\n",
      "300/300 - 2s - loss: 0.0351 - accuracy: 0.9901 - val_loss: 0.0694 - val_accuracy: 0.9789 - 2s/epoch - 6ms/step\n",
      "Epoch 6/10\n",
      "300/300 - 2s - loss: 0.0260 - accuracy: 0.9931 - val_loss: 0.0669 - val_accuracy: 0.9790 - 2s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "300/300 - 2s - loss: 0.0198 - accuracy: 0.9949 - val_loss: 0.0586 - val_accuracy: 0.9807 - 2s/epoch - 6ms/step\n",
      "Epoch 8/10\n",
      "300/300 - 2s - loss: 0.0133 - accuracy: 0.9972 - val_loss: 0.0614 - val_accuracy: 0.9820 - 2s/epoch - 6ms/step\n",
      "Epoch 9/10\n",
      "300/300 - 2s - loss: 0.0106 - accuracy: 0.9979 - val_loss: 0.0585 - val_accuracy: 0.9828 - 2s/epoch - 6ms/step\n",
      "Epoch 10/10\n",
      "300/300 - 2s - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0628 - val_accuracy: 0.9818 - 2s/epoch - 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe1a14b6a90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the test dataset is used to evaluate the model and a classification error rate is printed.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "You should see the output below. This very simple network defined in very few lines of code achieves a respectable error rate of 1.71%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 98.18%\n",
      "Baseline Error: 1.82%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Convolutional Neural Network for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1400/1*uAeANQIOQPqWZnnuH-VEyw.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen how to load the MNIST dataset and train a simple multi-layer perceptron model on it, it is time to develop a more sophisticated convolutional neural network or CNN model.\n",
    "\n",
    "Keras does provide a lot of capability for creating convolutional neural networks.\n",
    "\n",
    "In this section we will create a simple CNN for MNIST that demonstrates how to use all of the aspects of a modern CNN implementation, including Convolutional layers, Pooling layers and Dropout layers.\n",
    "\n",
    "The first step is to import the classes and functions needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to load the MNIST dataset and reshape it so that it is suitable for use training a CNN. In Keras, the layers used for two-dimensional convolutions expect pixel values with the dimensions [pixels][width][height][channels].\n",
    "\n",
    "Note, we are forcing so-called channels-last ordering for consistency in this example.\n",
    "\n",
    "In the case of RGB, the last dimension pixels would be 3 for the red, green and blue components and it would be like having 3 image inputs for every color image. In the case of MNIST where the pixel values are gray scale, the pixel dimension is set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][width][height][channels]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, it is a good idea to normalize the pixel values to the range 0 and 1 and one hot encode the output variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define our neural network model.\n",
    "\n",
    "Convolutional neural networks are more complex than standard multi-layer perceptrons, so we will start by using a simple structure to begin with that uses all of the elements for state of the art results. Below summarizes the network architecture.\n",
    "\n",
    "- The first hidden layer is a convolutional layer called a Convolution2D. The layer has 32 feature maps, which with the size of 5×5 and a rectifier activation function. This is the input layer, expecting images with the structure outline above [pixels][width][height].\n",
    "- Next we define a pooling layer that takes the max called MaxPooling2D. It is configured with a pool size of 2×2.\n",
    "- The next layer is a regularization layer using dropout called Dropout. It is configured to randomly exclude 20% of neurons in the layer in order to reduce overfitting.\n",
    "- Next is a layer that converts the 2D matrix data to a vector called Flatten. It allows the output to be processed by standard fully connected layers.\n",
    "- Next a fully connected layer with 128 neurons and rectifier activation function.\n",
    "- Finally, the output layer has 10 neurons for the 10 classes and a softmax activation function to output probability-like predictions for each class.\n",
    "\n",
    "As before, the model is trained using logarithmic loss and the ADAM gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple CNN model\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D())\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the model the same way as before with the multi-layer perceptron. The CNN is fit over 10 epochs with a batch size of 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = baseline_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example, the accuracy on the training and validation test is printed each epoch and at the end of the classification error rate is printed.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "Epochs may take about 45 seconds to run on the GPU (e.g. on AWS). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 [==============================] - 14s 47ms/step - loss: 0.2486 - accuracy: 0.9289 - val_loss: 0.0831 - val_accuracy: 0.9746\n",
      "Epoch 2/10\n",
      "300/300 [==============================] - 15s 51ms/step - loss: 0.0748 - accuracy: 0.9777 - val_loss: 0.0505 - val_accuracy: 0.9829\n",
      "Epoch 3/10\n",
      "300/300 [==============================] - 13s 44ms/step - loss: 0.0525 - accuracy: 0.9840 - val_loss: 0.0448 - val_accuracy: 0.9850\n",
      "Epoch 4/10\n",
      "300/300 [==============================] - 13s 44ms/step - loss: 0.0415 - accuracy: 0.9873 - val_loss: 0.0411 - val_accuracy: 0.9860\n",
      "Epoch 5/10\n",
      "300/300 [==============================] - 13s 44ms/step - loss: 0.0339 - accuracy: 0.9893 - val_loss: 0.0347 - val_accuracy: 0.9885\n",
      "Epoch 6/10\n",
      "300/300 [==============================] - 13s 43ms/step - loss: 0.0284 - accuracy: 0.9910 - val_loss: 0.0344 - val_accuracy: 0.9881\n",
      "Epoch 7/10\n",
      "300/300 [==============================] - 13s 44ms/step - loss: 0.0235 - accuracy: 0.9922 - val_loss: 0.0345 - val_accuracy: 0.9891\n",
      "Epoch 8/10\n",
      "300/300 [==============================] - 13s 43ms/step - loss: 0.0187 - accuracy: 0.9940 - val_loss: 0.0342 - val_accuracy: 0.9884\n",
      "Epoch 9/10\n",
      "300/300 [==============================] - 13s 43ms/step - loss: 0.0167 - accuracy: 0.9948 - val_loss: 0.0324 - val_accuracy: 0.9897\n",
      "Epoch 10/10\n",
      "300/300 [==============================] - 13s 43ms/step - loss: 0.0150 - accuracy: 0.9950 - val_loss: 0.0355 - val_accuracy: 0.9891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe198f9c4f0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, \n",
    "          validation_data=(X_test, y_test), \n",
    "          epochs=10, \n",
    "          batch_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the network achieves an error rate of 0.95%, which is better than our simple multi-layer perceptron model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Accuracy: 98.91%\n",
      "CNN Error: 1.09%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larger Convolutional Neural Network for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen how to create a simple CNN, let’s take a look at a model capable of close to state of the art results.\n",
    "\n",
    "We import classes and function then load and prepare the data the same as in the previous CNN example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Larger CNN for the MNIST Dataset\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we define a large CNN architecture with additional convolutional, max pooling layers and fully connected layers. The network topology can be summarized as follows.\n",
    "\n",
    "- Convolutional layer with 30 feature maps of size 5×5.\n",
    "- Pooling layer taking the max over 2*2 patches.\n",
    "- Convolutional layer with 15 feature maps of size 3×3.\n",
    "- Pooling layer taking the max over 2*2 patches.\n",
    "- Dropout layer with a probability of 20%.\n",
    "- Flatten layer.\n",
    "- Fully connected layer with 128 neurons and rectifier activation.\n",
    "- Fully connected layer with 50 neurons and rectifier activation.\n",
    "- Output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the larger model\n",
    "def larger_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D())\n",
    "\tmodel.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D())\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "\tmodel.add(Dense(50, activation='relu'))\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the previous two experiments, the model is fit over 10 epochs with a batch size of 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = larger_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example prints accuracy on the training and validation datasets each epoch and a final classification error rate.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "The model takes about 100 seconds to run per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 [==============================] - 16s 51ms/step - loss: 0.3824 - accuracy: 0.8821 - val_loss: 0.0778 - val_accuracy: 0.9751\n",
      "Epoch 2/10\n",
      "300/300 [==============================] - 15s 51ms/step - loss: 0.0962 - accuracy: 0.9707 - val_loss: 0.0514 - val_accuracy: 0.9844\n",
      "Epoch 3/10\n",
      "300/300 [==============================] - 15s 51ms/step - loss: 0.0683 - accuracy: 0.9793 - val_loss: 0.0383 - val_accuracy: 0.9872\n",
      "Epoch 4/10\n",
      "300/300 [==============================] - 15s 51ms/step - loss: 0.0545 - accuracy: 0.9835 - val_loss: 0.0343 - val_accuracy: 0.9884\n",
      "Epoch 5/10\n",
      "300/300 [==============================] - 15s 51ms/step - loss: 0.0468 - accuracy: 0.9858 - val_loss: 0.0334 - val_accuracy: 0.9888\n",
      "Epoch 6/10\n",
      "300/300 [==============================] - 15s 51ms/step - loss: 0.0445 - accuracy: 0.9860 - val_loss: 0.0279 - val_accuracy: 0.9902\n",
      "Epoch 7/10\n",
      "300/300 [==============================] - 15s 51ms/step - loss: 0.0383 - accuracy: 0.9880 - val_loss: 0.0266 - val_accuracy: 0.9914\n",
      "Epoch 8/10\n",
      "300/300 [==============================] - 15s 50ms/step - loss: 0.0346 - accuracy: 0.9887 - val_loss: 0.0253 - val_accuracy: 0.9923\n",
      "Epoch 9/10\n",
      "300/300 [==============================] - 15s 51ms/step - loss: 0.0325 - accuracy: 0.9898 - val_loss: 0.0242 - val_accuracy: 0.9923\n",
      "Epoch 10/10\n",
      "300/300 [==============================] - 15s 51ms/step - loss: 0.0307 - accuracy: 0.9902 - val_loss: 0.0268 - val_accuracy: 0.9915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe1975f53a0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This slightly larger model achieves the respectable classification error rate of 0.83%. This is not an optimized network topology. Nor is a reproduction of a network topology from a recent paper. There is a lot of opportunity for you to tune and improve upon this model.\n",
    "\n",
    "What is the best error rate score you can achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large CNN Accuracy: 99.15%\n",
      "Large CNN Error: 0.85%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Large CNN Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('models/cnn_model_3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a new prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction for a new image.\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_7='inputs/sample_image_7.png'\n",
    "image_3='inputs/sample_image_3.png'\n",
    "image_2='inputs/sample_image_2.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare the image\n",
    "def load_image(filename):\n",
    "\t# load the image\n",
    "\timg = load_img(filename, grayscale=True, target_size=(28, 28))\n",
    "\t# convert to array\n",
    "\timg = img_to_array(img)\n",
    "\t# reshape into a single sample with 1 channel\n",
    "\timg = img.reshape(1, 28, 28, 1)\n",
    "\t# prepare pixel data\n",
    "\timg = img.astype('float32')\n",
    "\timg = img / 255.0\n",
    "\treturn img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an image and predict the class\n",
    "def run_example(filename):\n",
    "\t# load the image\n",
    "\timg = load_image(filename)\n",
    "\t# load model\n",
    "\tmodel = load_model('models/cnn_model_3.h5')\n",
    "\t# predict the class\n",
    "\tpredict_value = model.predict(img)\n",
    "\tdigit = argmax(predict_value)\n",
    "\tprint(digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/keras_preprocessing/image/utils.py:107: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    }
   ],
   "source": [
    "# entry point, run the example\n",
    "run_example(image_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# entry point, run the example\n",
    "run_example(image_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# entry point, run the example\n",
    "run_example(image_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. More Complex Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST handwritten digit classification problem is a standard dataset used in computer vision and deep learning.\n",
    "\n",
    "Although the dataset is effectively solved, it can be used as the basis for learning and practicing how to develop, evaluate, and use convolutional deep learning neural networks for image classification from scratch. This includes how to develop a robust test harness for estimating the performance of the model, how to explore improvements to the model, and how to save the model and later load it to make predictions on new data.\n",
    "\n",
    "In this tutorial, you will discover how to develop a convolutional neural network for handwritten digit classification from scratch.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "    How to develop a test harness to develop a robust evaluation of a model and establish a baseline of performance for a classification task.\n",
    "    How to explore extensions to a baseline model to improve learning and model capacity.\n",
    "    How to develop a finalized model, evaluate the performance of the final model, and use it to make predictions on new images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation Methodology\n",
    "\n",
    "Although the MNIST dataset is effectively solved, it can be a useful starting point for developing and practicing a methodology for solving image classification tasks using convolutional neural networks.\n",
    "\n",
    "Instead of reviewing the literature on well-performing models on the dataset, we can develop a new model from scratch.\n",
    "\n",
    "The dataset already has a well-defined train and test dataset that we can use.\n",
    "\n",
    "In order to estimate the performance of a model for a given training run, we can further split the training set into a train and validation dataset. Performance on the train and validation dataset over each run can then be plotted to provide learning curves and insight into how well a model is learning the problem.\n",
    "\n",
    "The Keras API supports this by specifying the “validation_data” argument to the model.fit() function when training the model, that will, in turn, return an object that describes model performance for the chosen loss and metrics on each training epoch.\n",
    "\n",
    "In order to estimate the performance of a model on the problem in general, we can use k-fold cross-validation, perhaps five-fold cross-validation. This will give some account of the models variance with both respect to differences in the training and test datasets, and in terms of the stochastic nature of the learning algorithm. The performance of a model can be taken as the mean performance across k-folds, given the standard deviation, that could be used to estimate a confidence interval if desired.\n",
    "\n",
    "We can use the KFold class from the scikit-learn API to implement the k-fold cross-validation evaluation of a given neural network model. There are many ways to achieve this, although we can choose a flexible approach where the KFold class is only used to specify the row indexes used for each spit.\n",
    "\n",
    "We will hold back the actual test dataset and use it as an evaluation of our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "The first step is to develop a baseline model.\n",
    "\n",
    "This is critical as it both involves developing the infrastructure for the test harness so that any model we design can be evaluated on the dataset, and it establishes a baseline in model performance on the problem, by which all improvements can be compared.\n",
    "\n",
    "The design of the test harness is modular, and we can develop a separate function for each piece. This allows a given aspect of the test harness to be modified or inter-changed, if we desire, separately from the rest.\n",
    "\n",
    "We can develop this test harness with five key elements. They are the loading of the dataset, the preparation of the dataset, the definition of the model, the evaluation of the model, and the presentation of results.\n",
    "\n",
    "We know some things about the dataset.\n",
    "\n",
    "For example, we know that the images are all pre-aligned (e.g. each image only contains a hand-drawn digit), that the images all have the same square size of 28×28 pixels, and that the images are grayscale.\n",
    "\n",
    "Therefore, we can load the images and reshape the data arrays to have a single color channel.\n",
    "\n",
    "We also know that there are 10 classes and that classes are represented as unique integers.\n",
    "\n",
    "We can, therefore, use a one hot encoding for the class element of each sample, transforming the integer into a 10 element binary vector with a 1 for the index of the class value, and 0 values for all other classes. We can achieve this with the to_categorical() utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The load_dataset() function implements these behaviors and can be used to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "\t# load dataset\n",
    "\t(trainX, trainY), (testX, testY) = mnist.load_data()\n",
    "\t# reshape dataset to have a single channel\n",
    "\ttrainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
    "\ttestX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
    "\t# one hot encode target values\n",
    "\ttrainY = to_categorical(trainY)\n",
    "\ttestY = to_categorical(testY)\n",
    "\treturn trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Pixel Data\n",
    "\n",
    "We know that the pixel values for each image in the dataset are unsigned integers in the range between black and white, or 0 and 255.\n",
    "\n",
    "We do not know the best way to scale the pixel values for modeling, but we know that some scaling will be required.\n",
    "\n",
    "A good starting point is to normalize the pixel values of grayscale images, e.g. rescale them to the range [0,1]. This involves first converting the data type from unsigned integers to floats, then dividing the pixel values by the maximum value.\n",
    "\n",
    "The prep_pixels() function below implements these behaviors and is provided with the pixel values for both the train and test datasets that will need to be scaled.\n",
    "\n",
    "This function must be called to prepare the pixel values prior to any modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale pixels\n",
    "def prep_pixels(train, test):\n",
    "\t# convert from integers to floats\n",
    "\ttrain_norm = train.astype('float32')\n",
    "\ttest_norm = test.astype('float32')\n",
    "\t# normalize to range 0-1\n",
    "\ttrain_norm = train_norm / 255.0\n",
    "\ttest_norm = test_norm / 255.0\n",
    "\t# return normalized images\n",
    "\treturn train_norm, test_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model\n",
    "\n",
    "Next, we need to define a baseline convolutional neural network model for the problem.\n",
    "\n",
    "The model has two main aspects: the feature extraction front end comprised of convolutional and pooling layers, and the classifier backend that will make a prediction.\n",
    "\n",
    "For the convolutional front-end, we can start with a single convolutional layer with a small filter size (3,3) and a modest number of filters (32) followed by a max pooling layer. The filter maps can then be flattened to provide features to the classifier.\n",
    "\n",
    "Given that the problem is a multi-class classification task, we know that we will require an output layer with 10 nodes in order to predict the probability distribution of an image belonging to each of the 10 classes. This will also require the use of a softmax activation function. Between the feature extractor and the output layer, we can add a dense layer to interpret the features, in this case with 100 nodes.\n",
    "\n",
    "All layers will use the ReLU activation function and the He weight initialization scheme, both best practices.\n",
    "\n",
    "We will use a conservative configuration for the stochastic gradient descent optimizer with a learning rate of 0.01 and a momentum of 0.9. The categorical cross-entropy loss function will be optimized, suitable for multi-class classification, and we will monitor the classification accuracy metric, which is appropriate given we have the same number of examples in each of the 10 classes.\n",
    "\n",
    "The define_model() function below will define and return this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cnn model\n",
    "def define_model():\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Dense(10, activation='softmax'))\n",
    "\t# compile model\n",
    "\topt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model\n",
    "\n",
    "After the model is defined, we need to evaluate it.\n",
    "\n",
    "The model will be evaluated using five-fold cross-validation. The value of k=5 was chosen to provide a baseline for both repeated evaluation and to not be so large as to require a long running time. Each test set will be 20% of the training dataset, or about 12,000 examples, close to the size of the actual test set for this problem.\n",
    "\n",
    "The training dataset is shuffled prior to being split, and the sample shuffling is performed each time, so that any model we evaluate will have the same train and test datasets in each fold, providing an apples-to-apples comparison between models.\n",
    "\n",
    "We will train the baseline model for a modest 10 training epochs with a default batch size of 32 examples. The test set for each fold will be used to evaluate the model both during each epoch of the training run, so that we can later create learning curves, and at the end of the run, so that we can estimate the performance of the model. As such, we will keep track of the resulting history from each run, as well as the classification accuracy of the fold.\n",
    "\n",
    "The evaluate_model() function below implements these behaviors, taking the training dataset as arguments and returning a list of accuracy scores and training histories that can be later summarized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a model using k-fold cross-validation\n",
    "def evaluate_model(dataX, dataY, n_folds=5):\n",
    "\tscores, histories = list(), list()\n",
    "\t# prepare cross validation\n",
    "\tkfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "\t# enumerate splits\n",
    "\tfor train_ix, test_ix in kfold.split(dataX):\n",
    "\t\t# define model\n",
    "\t\tmodel = define_model()\n",
    "\t\t# select rows for train and test\n",
    "\t\ttrainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]\n",
    "\t\t# fit model\n",
    "\t\thistory = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)\n",
    "\t\t# evaluate model\n",
    "\t\t_, acc = model.evaluate(testX, testY, verbose=1)\n",
    "\t\tprint('> %.3f' % (acc * 100.0))\n",
    "\t\t# stores scores\n",
    "\t\tscores.append(acc)\n",
    "\t\thistories.append(history)\n",
    "\treturn scores, histories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Present Results\n",
    "\n",
    "Once the model has been evaluated, we can present the results.\n",
    "\n",
    "There are two key aspects to present: the diagnostics of the learning behavior of the model during training and the estimation of the model performance. These can be implemented using separate functions.\n",
    "\n",
    "First, the diagnostics involve creating a line plot showing model performance on the train and test set during each fold of the k-fold cross-validation. These plots are valuable for getting an idea of whether a model is overfitting, underfitting, or has a good fit for the dataset.\n",
    "\n",
    "We will create a single figure with two subplots, one for loss and one for accuracy. Blue lines will indicate model performance on the training dataset and orange lines will indicate performance on the hold out test dataset. The summarize_diagnostics() function below creates and shows this plot given the collected training histories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(histories):\n",
    "\tfor i in range(len(histories)):\n",
    "\t\t# plot loss\n",
    "\t\tplt.subplot(2, 1, 1)\n",
    "\t\tplt.title('Cross Entropy Loss')\n",
    "\t\tplt.plot(histories[i].history['loss'], color='blue', label='train')\n",
    "\t\tplt.plot(histories[i].history['val_loss'], color='orange', label='test')\n",
    "\t\t# plot accuracy\n",
    "\t\tplt.subplot(2, 1, 2)\n",
    "\t\tplt.title('Classification Accuracy')\n",
    "\t\tplt.plot(histories[i].history['accuracy'], color='blue', label='train')\n",
    "\t\tplt.plot(histories[i].history['val_accuracy'], color='orange', label='test')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the classification accuracy scores collected during each fold can be summarized by calculating the mean and standard deviation. This provides an estimate of the average expected performance of the model trained on this dataset, with an estimate of the average variance in the mean. We will also summarize the distribution of scores by creating and showing a box and whisker plot.\n",
    "\n",
    "The summarize_performance() function below implements this for a given list of scores collected during model evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize model performance\n",
    "def summarize_performance(scores):\n",
    "\t# print summary\n",
    "\tprint('Accuracy: mean=%.3f std=%.3f, n=%d' % (mean(scores)*100, std(scores)*100, len(scores)))\n",
    "\t# box and whisker plots of results\n",
    "\tplt.boxplot(scores)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test harness\n",
    "\n",
    "We need a function that will drive the test harness.\n",
    "\n",
    "This involves calling all of the define functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# load dataset\n",
    "\ttrainX, trainY, testX, testY = load_dataset()\n",
    "\t# prepare pixel data\n",
    "\ttrainX, testX = prep_pixels(trainX, testX)\n",
    "\t# evaluate model\n",
    "\tscores, histories = evaluate_model(trainX, trainY)\n",
    "\t# learning curves\n",
    "\tsummarize_diagnostics(histories)\n",
    "\t# summarize estimated performance\n",
    "\tsummarize_performance(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything we need; the complete code example for a baseline convolutional neural network model on the MNIST dataset is listed below.\n",
    "\n",
    "### Step 1\n",
    "\n",
    "Running the example prints the classification accuracy for each fold of the cross-validation process. This is helpful to get an idea that the model evaluation is progressing.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "We can see two cases where the model achieves perfect skill and one case where it achieved lower than 98% accuracy. These are good results.\n",
    "\n",
    "### Step 2\n",
    "\n",
    "Next, a diagnostic plot is shown, giving insight into the learning behavior of the model across each fold.\n",
    "\n",
    "In this case, we can see that the model generally achieves a good fit, with train and test learning curves converging. There is no obvious sign of over- or underfitting.\n",
    "\n",
    "### Step 3\n",
    "\n",
    "Next, a summary of the model performance is calculated.\n",
    "\n",
    "We can see in this case, the model has an estimated skill of about 98.6%, which is reasonable.\n",
    "\n",
    "### Step 4\n",
    "\n",
    "Finally, a box and whisker plot is created to summarize the distribution of accuracy scores.\n",
    "\n",
    "We now have a robust test harness and a well-performing baseline model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0619 - accuracy: 0.9845\n",
      "> 98.450\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0509 - accuracy: 0.9874\n",
      "> 98.742\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0539 - accuracy: 0.9869\n",
      "> 98.692\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0426 - accuracy: 0.9885\n",
      "> 98.850\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0565 - accuracy: 0.9874\n",
      "> 98.742\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: mean=98.695 std=0.133, n=5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to Develop an Improved Model\n",
    "\n",
    "There are many ways that we might explore improvements to the baseline model.\n",
    "\n",
    "We will look at areas of model configuration that often result in an improvement, so-called low-hanging fruit. The first is a change to the learning algorithm, and the second is an increase in the depth of the model.\n",
    "Improvement to Learning\n",
    "\n",
    "There are many aspects of the learning algorithm that can be explored for improvement.\n",
    "\n",
    "Perhaps the point of biggest leverage is the learning rate, such as evaluating the impact that smaller or larger values of the learning rate may have, as well as schedules that change the learning rate during training.\n",
    "\n",
    "Another approach that can rapidly accelerate the learning of a model and can result in large performance improvements is batch normalization. We will evaluate the effect that batch normalization has on our baseline model.\n",
    "\n",
    "Batch normalization can be used after convolutional and fully connected layers. It has the effect of changing the distribution of the output of the layer, specifically by standardizing the outputs. This has the effect of stabilizing and accelerating the learning process.\n",
    "\n",
    "We can update the model definition to use batch normalization after the activation function for the convolutional and dense layers of our baseline model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('models/cnn_model_4.h5')"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
